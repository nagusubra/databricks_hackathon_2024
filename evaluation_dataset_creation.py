# Databricks notebook source
# MAGIC %pip install mlflow==2.10.1 lxml==4.9.3 langchain==0.1.5 databricks-vectorsearch==0.22 cloudpickle==2.2.1 databricks-sdk==0.18.0 cloudpickle==2.2.1 pydantic==2.5.2 transformers==4.30.2 "unstructured[pdf,docx]==0.10.30" llama-index==0.9.3
# MAGIC %pip install pip mlflow[databricks]==2.10.1
# MAGIC
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC #Import libraries and modules

# COMMAND ----------

# MAGIC %run /Workspace/Repos/subramanian.narayana.ucalgary@gmail.com/databricks_hackathon_2024/_resources/00-init-advanced $reset_all_data=false

# COMMAND ----------

install_ocr_on_nodes()

# COMMAND ----------

from unstructured.partition.auto import partition
import re
from llama_index.langchain_helpers.text_splitter import SentenceSplitter
from llama_index import Document, set_global_tokenizer
from transformers import AutoTokenizer
from typing import Iterator
from mlflow.deployments import get_deploy_client
import io
from databricks.vector_search.client import VectorSearchClient
from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c

from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatDatabricks
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableLambda
from operator import itemgetter
from databricks.vector_search.client import VectorSearchClient
from langchain_community.vectorstores import DatabricksVectorSearch
from langchain_community.embeddings import DatabricksEmbeddings
from langchain.chains import RetrievalQA
from langchain.schema.runnable import RunnableBranch
from langchain.schema.runnable import RunnableBranch, RunnableParallel, RunnablePassthrough
import json


from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, DoubleType, TimestampType, StringType
from pyspark.sql.functions import udf
from langchain.schema.output_parser import StrOutputParser

# COMMAND ----------

# MAGIC %md
# MAGIC #Import constants

# COMMAND ----------

volume_folder =  f"/Volumes/{catalog}/{db}/volume_oem_documentation"
os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get("dbdemos", "rag_sp_token")

# COMMAND ----------

# MAGIC %md
# MAGIC #Create OEM documentation volume storage

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE VOLUME IF NOT EXISTS volume_oem_documentation;

# COMMAND ----------

# MAGIC %md
# MAGIC #PDF chunking with sentance splitter

# COMMAND ----------

def extract_doc_text(x : bytes) -> str:
  # Read files and extract the values with unstructured
  sections = partition(file=io.BytesIO(x))
  def clean_section(txt):
    txt = re.sub(r'\n', '', txt)
    return re.sub(r' ?\.', '.', txt)
  # Default split is by section of document, concatenate them all together because we want to split by sentence instead.
  return "\n".join([clean_section(s.text) for s in sections])


# Reduce the arrow batch size as our PDF can be big in memory
spark.conf.set("spark.sql.execution.arrow.maxRecordsPerBatch", 10)

@pandas_udf("array<string>")
def read_as_small_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:
    #set llama2 as tokenizer to match our model size (will stay below BGE 1024 limit)
    set_global_tokenizer(
      AutoTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer")
    )
    #Sentence splitter from llama_index to split on sentences
    splitter = SentenceSplitter(chunk_size=250, chunk_overlap=10)
    def extract_and_split(b):
      txt = extract_doc_text(b)
      nodes = splitter.get_nodes_from_documents([Document(text=txt)])
      return [n.text for n in nodes]

    for x in batch_iter:
        yield x.apply(extract_and_split)

# COMMAND ----------

# MAGIC %md
# MAGIC #Create pdf_evaluation

# COMMAND ----------

# MAGIC %sql
# MAGIC DROP TABLE main.asset_nav.pdf_evaluation

# COMMAND ----------

dbutils.fs.rm('dbfs:/Volumes/main/asset_nav/volume_oem_documentation/checkpoints/pdf_eval_chunk', True)

# COMMAND ----------

# MAGIC %sql
# MAGIC --Note that we need to enable Change Data Feed on the table to create the index
# MAGIC CREATE TABLE IF NOT EXISTS pdf_evaluation (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   url STRING,
# MAGIC   content STRING,
# MAGIC   question STRING,
# MAGIC   answer STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true);

# COMMAND ----------

# MAGIC %md
# MAGIC #Populate pdf_evaluation table

# COMMAND ----------

(spark.readStream.table('pdf_raw')
      .withColumn("content", F.explode(read_as_small_chunk("content")))
      .selectExpr('path as url', 'content')
  .writeStream
    .trigger(availableNow=True)
    .option("checkpointLocation", f'dbfs:{volume_folder}/checkpoints/pdf_eval_chunk')
    .table('pdf_evaluation').awaitTermination())

# COMMAND ----------

# MAGIC %md
# MAGIC ##Add questions

# COMMAND ----------

df = spark.sql("Select * from pdf_evaluation LIMIT 10")
# df = df.withColumn("question_1", (F.concat(F.lit("Hello       "), df["content"])))
display(df)

# COMMAND ----------

# MAGIC %md
# MAGIC ###LLM chat model and embedding model selection

# COMMAND ----------

chat_model = ChatDatabricks(endpoint="databricks-mixtral-8x7b-instruct", max_tokens = 200) # 1/4 the cost of DBRX, but accuracy and performance not better than DBRX and matches the performance of llama 3
embedding_model = DatabricksEmbeddings(endpoint="databricks-bge-large-en")

# COMMAND ----------

# MAGIC %md
# MAGIC ###Connecting to vector store

# COMMAND ----------

index_name=f"{catalog}.{db}.pdf_transformed_self_managed_vs_index"
host = "https://" + spark.conf.get("spark.databricks.workspaceUrl")

#Let's make sure the secret is properly setup and can access our vector search index. Check the quick-start demo for more guidance
test_demo_permissions(host, secret_scope="dbdemos", secret_key="rag_sp_token", vs_endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME, index_name=index_name, embedding_endpoint_name="databricks-bge-large-en", managed_embeddings = False)

# COMMAND ----------

print(VECTOR_SEARCH_ENDPOINT_NAME)
print(index_name)

# COMMAND ----------

# MAGIC %md
# MAGIC ###Langchains for question and answers 

# COMMAND ----------

# MAGIC %md
# MAGIC ####Connecting to vector store and vector search endpoint created for this project

# COMMAND ----------

def get_retriever(persist_dir: str = None):
    os.environ["DATABRICKS_HOST"] = host
    #Get the vector search index
    vsc = VectorSearchClient(workspace_url=host, personal_access_token=os.environ["DATABRICKS_TOKEN"])
    vs_index = vsc.get_index(
        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,
        index_name=index_name
    )

    # Create the retriever
    vectorstore = DatabricksVectorSearch(
        vs_index, text_column="content", embedding=embedding_model, columns=["url"]
    )
    return vectorstore.as_retriever(search_kwargs={'k': 4})

retriever = get_retriever()

# COMMAND ----------

#The question is the last entry of the history
def extract_question(input):
    return input[-1]["content"]

#The history is everything before the last question
def extract_history(input):
    return input[:-1]

# COMMAND ----------

bare_prompt_template = """
                            {content}
                       """

bare_template = PromptTemplate(
    input_variables = ["content"],
    template=bare_prompt_template
)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Question

# COMMAND ----------

qa_template = """
You are a University Professor creating a test for advanced students in solar energy. For each given context, create a descriptive question that is specific to the context. Avoid creating generic or general questions. Avoid out of context questions.

question: a question about the context.

Format the output as JSON with the following keys:
question

context: {context}
"""

qa_question_prompt_template = PromptTemplate(
    input_variables = ["context"],
    template=qa_template,
)

# COMMAND ----------

messages = qa_question_prompt_template.format(
    context=["context"],
    format_instructions='''
                            The output should be a markdown code snippet formatted in the following schema, including the leading and trailing "```json" and "```":

                            ```json
                            {
                                "question": string  // a question about the context.
                            }
                            ```
                        '''
)

question_generation_chain = (bare_template | chat_model |  StrOutputParser())
response = question_generation_chain.invoke({"content" : messages})
response

# COMMAND ----------

# MAGIC %md
# MAGIC ##### Answer

# COMMAND ----------

qa_answer_template = """
You are a University Professor creating a test for advanced students in solar energy. For each question and context, create a descriptive answer. Avoid creating generic or general answers. Avoid out of context answers.

answer: a answer about the context.

Format the output as JSON with the following keys:
answer

question: {question}
context: {context}
"""

qa_answer_prompt_template = PromptTemplate(
    input_variables = ["question", "context"],
    template=qa_answer_template,
)

# COMMAND ----------

messages = qa_answer_prompt_template.format(
    context=["context"],
    question=''' \n{\n"question": "In what situations might a \'Device restart\' be recommended, as suggested in the provided context?"\n} ''',
    format_instructions='''
                            The output should be a markdown code snippet formatted in the following schema, including the leading and trailing "```json" and "```":

                            ```json
                            {
                                "answer": string  // an answer to the question
                            }
                            ```
                        '''
)

answer_generation_chain = (bare_template | chat_model |  StrOutputParser())
response = answer_generation_chain.invoke({"content" : messages})
response

# COMMAND ----------

# MAGIC %md
# MAGIC #### Question function

# COMMAND ----------

# @pandas_udf("array<string>")
import pandas as pd
def generate_questions_str(contents: pd.Series) -> pd.Series:
    
    messages = qa_question_prompt_template.format(
        context=contents,
        format_instructions='''
                                The output should be a markdown code snippet formatted in the following schema, including the leading and trailing "```json" and "```":

                                ```json
                                {
                                    "question": string  // a question about the context.
                                }
                                ```
                            '''
    )

    question_generation_chain = (bare_template | chat_model | StrOutputParser())
    response = question_generation_chain.invoke({"content" : messages})
    return response

# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------

def rows_wise_string_func(str_val):
    x = "Hello      " + str(str_val)
    return x

@pandas_udf("array<string>")
def col_wise_string_func(str_col: pd.Series) -> pd.Series:
    
    # Splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.
    max_batch_size = 150
    batches = [str_col.iloc[i:i + max_batch_size] for i in range(0, len(str_col), max_batch_size)]

    # Process each batch and collect the results
    return_col = []
    for batch in batches:
        # return_col += get_embeddings(batch.tolist())
        return_col += "Hello   " + str(batch)

    return pd.Series(return_col)

# COMMAND ----------


df = spark.sql("Select * from pdf_evaluation LIMIT 10")
# df = df.withColumn("question_1", (F.concat(F.lit("Hello       "), df["content"])))
display(df)

# COMMAND ----------

def add_hello(val):
    x = "Hello    " + str(val)
    return x

add_hello_udf = udf(add_hello, StringType())
add_questions_udf = udf(generate_questions_str, StringType())

# COMMAND ----------

df = df.withColumn("content_question___fdsdfsfsdfsf_", add_questions_udf(df["content"]))
display(df)

# COMMAND ----------

from pyspark.sql import functions as F

df = spark.sql("Select * from pdf_evaluation LIMIT 10")
df = df.withColumn("question_1", col_wise_string_func(F.concat(F.lit("Hello       "), df["content"])))
display(df)

# COMMAND ----------



# COMMAND ----------



# COMMAND ----------

# context_str ='''
#                     [Document(page_content='Device restart attempted? (Disconnect AC/DC – switch it back on after 5 minutes.) => If not successful, contact the service department.\n190 Semiconductor module\nSee explanation in status 189\nSee remedy in status 189\n2 in channel A defect- ive\n191 Semiconductor module\nSee explanation in status 189\nSee remedy in status 189\n3 in channel A defect- ive\n192 Semiconductor module\n1 in channel B defect- ive\nSemiconductor module x from channel B is defective or the corresponding filter re- lay is not closing correctly.\nDevice restart attempted? (Disconnect AC/DC – switch it back on after 5 minutes.) => If not successful, contact the service department.\n193 Semiconductor module\nSee explanation in status 191\nSee remedy in status 191\n2 in channel B defect- ive\n194 Semiconductor module\nSee explanation in status 191\nSee remedy in status 191\n3 in channel B defect- ive\n195 DESAT error\nSaturation monitoring (DESAT) of the IG- BTs has tripped or the voltage supply (24 V) of the gate drivers is too low.\nIn case of repeated occurrence, contact the service department.\n203 Protection shutdown\ngrid volt. L1\nProtective shutdown due to excessive grid voltage. It is the effective value which is decisive for the shutdown pro- cedure.\nIs the error displayed frequently? => Check the installation again. Otherwise contact the service department. Note: Fault can be caused by a poor loca- tion on the AC side. Check all connection terminals from the inverter to the grid connection. A fluctuating or missing AC voltage can indicate this connection prob- lem.\n204 Protection shutdown\nSee explanation in status 203\nSee remedy in status 203\ngrid volt.', metadata={'url': 'dbfs:/Volumes/main/asset_nav/volume_oem_documentation/input_data/BP150_Manual_Kaco blueplanet_ Event codes - Vendor status codes - good.pdf', 'id': 264.0}), Document(page_content='Device restart attempted? (Disconnect AC/DC – switch it back on after 5 minutes.) => If not successful, contact the service department.\n190 Semiconductor module\nSee explanation in status 189\nSee remedy in status 189\n2 in channel A defect- ive\n191 Semiconductor module\nSee explanation in status 189\nSee remedy in status 189\n3 in channel A defect- ive\n192 Semiconductor module\n1 in channel B defect- ive\nSemiconductor module x from channel B is defective or the corresponding filter re- lay is not closing correctly.\nDevice restart attempted? (Disconnect AC/DC – switch it back on after 5 minutes.) => If not successful, contact the service department.\n193 Semiconductor module\nSee explanation in status 191\nSee remedy in status 191\n2 in channel B defect- ive\n194 Semiconductor module\nSee explanation in status 191\nSee remedy in status 191\n3 in channel B defect- ive\n195 DESAT error\nSaturation monitoring (DESAT) of the IG- BTs has tripped or the voltage supply (24 V) of the gate drivers is too low.\nIn case of repeated occurrence, contact the service department.\n203 Protection shutdown\ngrid volt. L1\nProtective shutdown due to excessive grid voltage. It is the effective value which is decisive for the shutdown pro- cedure.\nIs the error displayed frequently? => Check the installation again. Otherwise contact the service department. Note: Fault can be caused by a poor loca- tion on the AC side. Check all connection terminals from the inverter to the grid connection. A fluctuating or missing AC voltage can indicate this connection prob- lem.\n204 Protection shutdown\nSee explanation in status 203\nSee remedy in status 203\ngrid volt.', metadata={'url': 'dbfs:/Volumes/main/asset_nav/volume_oem_documentation/input_data/BP150_Manual_Kaco blueplanet_1-9-2023 DCB - Manual extended - medium.pdf', 'id': 218.0}), Document(page_content='This error can be caused by a poor con- nection/cabling on the AC side. Check all connection terminals from the inverter to the grid connection. A fluctuating or miss- ing AC voltage can indicate this connec- tion problem.\n48\nGrid failure underfre- quency\nMeasured value for grid frequency is out- side the permissible limit. Limit is coun- try-dependent.\nHas the correct country setting been made? => Check set values in the para- meters menu. Your installer should check the AC con- nection and the connection to the device. Otherwise, contact the service depart- ment.\n49\nGrid failure overfre- quency\nSee explanation in status 48\nSee remedy in status 48\n50\nGrid failure: average voltage\nThe grid voltage measurement according to EN 50160 (10 min average value) has exceeded the maximum permitted limit value.\nIf it appears frequently, check the settings in the menu. > Has the firmware not been installed correctly? => Unpack the firmware ac- cording to the instructions in the down- load area.\n56\nSPI remote shutdown\nRemote switch off via digital input. (e.g. CEI 0-21)\nThis was caused by the grid operator.\n57 Waiting for reactiva-\ntion\nThe waiting period following a fault is country-dependent and can last several minutes.\nDoes the message appear frequently? => Find the reason for switching off via error logs, Prolog status\n58\nControl board over- temp.\nThe temperature inside the device was too high. The device shuts down to avoid hardware damage.\nThis message appears only at high ambi- ent temperatures (<60°C) > Has the power been checked? => Ob- serve derating temperature according to data sheet > Is the fan or heat sink covered? => Clean device according to chapter 11.2.\n59\nSelf test error\nA fault occurred during a self-test.', metadata={'url': 'dbfs:/Volumes/main/asset_nav/volume_oem_documentation/input_data/BP150_Manual_Kaco blueplanet_1-9-2023 DCB - Manual extended - medium.pdf', 'id': 205.0}), Document(page_content='This error can be caused by a poor con- nection/cabling on the AC side. Check all connection terminals from the inverter to the grid connection. A fluctuating or miss- ing AC voltage can indicate this connec- tion problem.\n48\nGrid failure underfre- quency\nMeasured value for grid frequency is out- side the permissible limit. Limit is coun- try-dependent.\nHas the correct country setting been made? => Check set values in the para- meters menu. Your installer should check the AC con- nection and the connection to the device. Otherwise, contact the service depart- ment.\n49\nGrid failure overfre- quency\nSee explanation in status 48\nSee remedy in status 48\n50\nGrid failure: average voltage\nThe grid voltage measurement according to EN 50160 (10 min average value) has exceeded the maximum permitted limit value.\nIf it appears frequently, check the settings in the menu. > Has the firmware not been installed correctly? => Unpack the firmware ac- cording to the instructions in the down- load area.\n56\nSPI remote shutdown\nRemote switch off via digital input. (e.g. CEI 0-21)\nThis was caused by the grid operator.\n57 Waiting for reactiva-\ntion\nThe waiting period following a fault is country-dependent and can last several minutes.\nDoes the message appear frequently? => Find the reason for switching off via error logs, Prolog status\n58\nControl board over- temp.\nThe temperature inside the device was too high. The device shuts down to avoid hardware damage.\nThis message appears only at high ambi- ent temperatures (<60°C) > Has the power been checked? => Ob- serve derating temperature according to data sheet > Is the fan or heat sink covered? => Clean device according to chapter 11.2.\n59\nSelf test error\nA fault occurred during a self-test.', metadata={'url': 'dbfs:/Volumes/main/asset_nav/volume_oem_documentation/input_data/BP150_Manual_Kaco blueplanet_ Event codes - Vendor status codes - good.pdf', 'id': 251.0})]
#             ''',
    

# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------

# from llama_index.langchain_helpers.text_splitter import SentenceSplitter
# from llama_index import Document, set_global_tokenizer
# from transformers import AutoTokenizer
# from typing import Iterator

# # Reduce the arrow batch size as our PDF can be big in memory
# spark.conf.set("spark.sql.execution.arrow.maxRecordsPerBatch", 10)

# @pandas_udf("array<string>")
# def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:
#     #set llama2 as tokenizer to match our model size (will stay below BGE 1024 limit)
#     set_global_tokenizer(
#       AutoTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer")
#     )
#     #Sentence splitter from llama_index to split on sentences
#     splitter = SentenceSplitter(chunk_size=200, chunk_overlap=10)
#     def extract_and_split(b):
#       txt = extract_doc_text(b)
#       nodes = splitter.get_nodes_from_documents([Document(text=txt)])
#       return [n.text for n in nodes]

#     for x in batch_iter:
#         yield x.apply(extract_and_split)

# COMMAND ----------

# from unstructured.partition.auto import partition
# import io
# import re

# def extract_doc_text(x : bytes) -> str:
#   # Read files and extract the values with unstructured
#   sections = partition(file=io.BytesIO(x))
#   def clean_section(txt):
#     txt = re.sub(r'\n', '', txt)
#     return re.sub(r' ?\.', '.', txt)
#   # Default split is by section of document, concatenate them all together because we want to split by sentence instead.
#   return "\n".join([clean_section(s.text) for s in sections])

# COMMAND ----------

# from llama_index import Document

# page_content = "This is the content of my page."
# document = Document(text=page_content)
# document

# COMMAND ----------

# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from llama_index import Document

# splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)

# val = splitter.split_documents([Document(page_content="HELLO")])


# COMMAND ----------

# @pandas_udf("array<string>")
import pandas as pd

## if it doesn't work lets use this : 
# from pyspark.sql.types import ArrayType, StringType
# @udf(ArrayType(StringType())) 


def generate_questions_str(contents: pd.Series) -> pd.Series:
    
    messages = qa_question_prompt_template.format(
        context=contents,
        format_instructions='''
                                The output should be a markdown code snippet formatted in the following schema, including the leading and trailing "```json" and "```":

                                ```json
                                {
                                    "question": string  // a question about the context.
                                }
                                ```
                            '''
    )

    question_generation_chain = (bare_template | chat_model | StrOutputParser())
    response = question_generation_chain.invoke({"content" : messages})
    return response

# COMMAND ----------

@pandas_udf("array<string>")
def generate_questions(contents: pd.Series) -> pd.Series:
    
    messages = qa_question_prompt_template.format(
        context=contents,
        format_instructions='''
                                The output should be a markdown code snippet formatted in the following schema, including the leading and trailing "```json" and "```":

                                ```json
                                {
                                    "question": string  // a question about the context.
                                }
                                ```
                            '''
    )

    question_generation_chain = (bare_template | chat_model | StrOutputParser())
    response = question_generation_chain.invoke({"content" : messages})
    return response


@pandas_udf("array<string>")
def generate_answers(contents: pd.Series) -> pd.Series:
    
    messages = qa_answer_prompt_template.format(
        context=contents,
        question=''' \n{\n"question": "In what situations might a \'Device restart\' be recommended, as suggested in the provided context?"\n} ''',
        format_instructions='''
                                The output should be a markdown code snippet formatted in the following schema, including the leading and trailing "```json" and "```":

                                ```json
                                {
                                    "answer": string  // an answer to the question
                                }
                                ```
                            '''
    )

    answer_generation_chain = (bare_template | chat_model | StrOutputParser())
    response = answer_generation_chain.invoke({"content" : messages})
    return response

# COMMAND ----------

# MAGIC %sql
# MAGIC --Note that we need to enable Change Data Feed on the table to create the index
# MAGIC CREATE TABLE IF NOT EXISTS pdf_content_question_answer_for_eval (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   url STRING,
# MAGIC   content STRING,
# MAGIC   question STRING,
# MAGIC   answer STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true);

# COMMAND ----------

volume_folder =  f"/Volumes/{catalog}/{db}/volume_oem_documentation"
print(volume_folder)
(spark.readStream.table('pdf_raw')
      .withColumn("content", F.explode(read_as_chunk("content")))
      .withColumn("question", generate_questions("content"))
      .withColumn("answer", generate_answers("question"))
      .selectExpr('path as url', 'content')
  .writeStream
    .trigger(availableNow=True)
    .option("checkpointLocation", f'dbfs:{volume_folder}/checkpoints/pdf_content_question_answer_for_eval')
    .table('pdf_content_question_answer_for_eval').awaitTermination())

# COMMAND ----------

x = 'dbfs:/Volumes/main/asset_nav/volume_oem_documentation/checkpoints/pdf_content_question_answer_for_eval'
x

# COMMAND ----------

# dbutils.fs.rm('dbfs:/Volumes/main/asset_nav/volume_oem_documentation/checkpoints/pdf_chunk_content_question_answer_for_eval', True)
