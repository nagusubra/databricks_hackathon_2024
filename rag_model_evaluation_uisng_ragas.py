# Databricks notebook source
# MAGIC %pip install -q mlflow[databricks]==2.10.1 lxml==4.9.3 databricks-vectorsearch==0.22 cloudpickle==2.2.1 databricks-sdk==0.18.0 cloudpickle==2.2.1 pydantic==2.5.2 transformers==4.30.2 unstructured[pdf,docx]==0.10.30 llama-index==0.9.3 langchain openai ragas arxiv pymupdf chromadb wandb tiktoken datasets mlflow==2.10.1 datasets tqdm
# MAGIC
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %run /Workspace/Repos/subramanian.narayana.ucalgary@gmail.com/databricks_hackathon_2024/_resources/00-init-advanced $reset_all_data=false

# COMMAND ----------

from pyspark.sql.functions import col

# COMMAND ----------

# MAGIC %md
# MAGIC #Creating OpenAI ChatGPT 3.5 as the judge for evaluation by setting up an endpoint
# MAGIC ---> Since we cant create an endpoint now, it defaults to llama2-70-B as the judge

# COMMAND ----------

from mlflow.deployments import get_deploy_client
deploy_client = get_deploy_client("databricks")

try:
    endpoint_name  = "dbdemos-azure-openai"
    deploy_client.create_endpoint(
        name=endpoint_name,
        config={
            "served_entities": [
                {
                    "name": endpoint_name,
                    "external_model": {
                        "name": "gpt-35-turbo",
                        "provider": "openai",
                        "task": "llm/v1/chat",
                        "openai_config": {
                            "openai_api_type": "azure",
                            "openai_api_key": "{{secrets/dbdemos/azure-openai}}", #Replace with your own azure open ai key
                            "openai_deployment_name": "dbdemo-gpt35",
                            "openai_api_base": "https://dbdemos-open-ai.openai.azure.com/",
                            "openai_api_version": "2023-05-15"
                        }
                    }
                }
            ]
        }
    )
except Exception as e:
    if 'RESOURCE_ALREADY_EXISTS' in str(e):
        print('Endpoint already exists')
    else:
        print(f"Couldn't create the external endpoint with Azure OpenAI: {e}. Will fallback to databricks-dbrx-instruct as judge. Consider using a stronger model as a judge.")
        endpoint_name = "databricks-dbrx-instruct"

#Let's query our external model endpoint
answer_test = deploy_client.predict(endpoint=endpoint_name, inputs={"messages": [{"role": "user", "content": "What is Apache Spark?"}]})
answer_test['choices'][0]['message']['content']

# COMMAND ----------

# MAGIC %md
# MAGIC #RAF model evaluation - offline

# COMMAND ----------

# %sql
# DROP TABLE main.asset_nav.pdf_evaluation_clean

# COMMAND ----------

# dbutils.fs.rm('dbfs:/Volumes/main/asset_nav/volume_oem_documentation/checkpoints/pdf_evaluation_clean_chunk', True)

# COMMAND ----------

# MAGIC %sql
# MAGIC --Note that we need to enable Change Data Feed on the table to create the index
# MAGIC CREATE TABLE IF NOT EXISTS main.asset_nav.pdf_evaluation_clean (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   question STRING,
# MAGIC   expected_answer STRING,
# MAGIC   predicted_answer STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true);

# COMMAND ----------

evaluation_dataset_path = "/Volumes/main/asset_nav/volume_oem_documentation/evaluation_data/solar_rag_pipeline_evaluation_datatset_manual.csv"
df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load(evaluation_dataset_path)
df = df.withColumn("id", col("id").cast("bigint"))
display(df)

df.write.format("delta").mode("overwrite").option("mergeSchema", "true").saveAsTable("main.asset_nav.pdf_evaluation_clean")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Automated Evaluation of our chatbot model registered in Unity Catalog
# MAGIC
# MAGIC Let's retrieve the chatbot model we registered in Unity Catalog and predict answers for each questions in the evaluation set.

# COMMAND ----------

import mlflow
os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get("dbdemos", "rag_sp_token")
model_name = f"{catalog}.{db}.asset_nav_chatbot_model_version_1"
model_version_to_evaluate = get_latest_model_version(model_name)
mlflow.set_registry_uri("databricks-uc")
rag_model = mlflow.langchain.load_model(f"models:/{model_name}/{model_version_to_evaluate}")

@pandas_udf("string")
def predict_answer(questions):
    def answer_question(question):
        dialog = {"messages": [{"role": "user", "content": question}]}
        return rag_model.invoke(dialog)['result']
    return questions.apply(answer_question)

# COMMAND ----------

df_qa = (spark.read.table('main.asset_nav.pdf_evaluation_clean')
                  .selectExpr('question as inputs', 'expected_answer as targets')
                  .where("targets is not null")
                  # .sample(fraction=0.005, seed=40) # .sample(fraction=0.005, seed=40): This line takes a random sample from the DataFrame. It randomly selects a fraction (0.5%) of the rows for the sample. The seed=40 parameter ensures that the sample is reproducible by setting a specific seed value.
        ) #small sample for interactive demo: This comment indicates that the sample size chosen (0.5%) is small for the purpose of an interactive demonstration.

df_qa_with_preds = df_qa.withColumn('preds', predict_answer(col('inputs'))).cache()
display(df_qa_with_preds)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ##LLMs-as-a-judge: automated LLM evaluation with out of the box and custom GenAI metrics
# MAGIC

# COMMAND ----------

from mlflow.metrics.genai.metric_definitions import answer_correctness
from mlflow.metrics.genai import make_genai_metric, EvaluationExample

# Because we have our labels (answers) within the evaluation dataset, we can evaluate the answer correctness as part of our metric. Again, this is optional.
answer_correctness_metrics = answer_correctness(model=f"endpoints:/{endpoint_name}")
print(answer_correctness_metrics)

# COMMAND ----------

# MAGIC %md
# MAGIC ##Metrics templates

# COMMAND ----------

professionalism_example = EvaluationExample(
    input="What is an Inverter?",
    output=(
        # "MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps "
        # "you track experiments, package your code and models, and collaborate with your team, making the whole ML "
        # "workflow smoother. It's like your Swiss Army knife for machine learning!"
        ''' 
            Oh, dude, so, like, an inverter is this super cool thing in solar energy setups! It's basically this device that takes the direct current (DC) electricity generated by solar panels and flips it into alternating current (AC), which is what we use in our homes and stuff, you know? It's like the middleman between your solar panels and your appliances, making sure everything runs smoothly and you can use all that awesome solar power to run your gadgets and keep the lights on!
        '''
    ),
    score=2,
    justification=(
        "The response is written in a casual tone. It uses contractions, filler words such as 'like', and "
        "exclamation points, which make it sound less professional. "
    )
)

professionalism = make_genai_metric(
    name="professionalism",
    definition=(
        "Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is "
        "tailored to the context and audience. It often involves avoiding overly casual language, slang, or "
        "colloquialisms, and instead using clear, concise, and respectful language."
    ),
    grading_prompt=(
        "Professionalism: If the answer is written using a professional tone, below are the details for different scores: "
        "- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for "
        "professional contexts."
        "- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in "
        "some informal professional settings."
        "- Score 3: Language is overall formal but still have casual words/phrases. Borderline for professional contexts."
        "- Score 4: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. "
        "- Score 5: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal "
        "business or academic settings. "
    ),
    model=f"endpoints:/{endpoint_name}",
    parameters={"temperature": 0.0},
    aggregations=["mean", "variance"],
    examples=[professionalism_example],
    greater_is_better=True
)

print(professionalism)

# COMMAND ----------

from mlflow.deployments import set_deployments_target

set_deployments_target("databricks")

#This will automatically log all
with mlflow.start_run(run_name="asset_nav") as run:
    eval_results = mlflow.evaluate(data = df_qa_with_preds.toPandas(), # evaluation data,
                                   model_type="question-answering", # toxicity and token_count will be evaluated   
                                   predictions="preds", # prediction column_name from eval_df
                                   targets = "targets",
                                   extra_metrics=[answer_correctness_metrics, professionalism])
    
eval_results.metrics

# COMMAND ----------

df_genai_metrics = eval_results.tables["eval_results_table"]
display(df_genai_metrics)

# COMMAND ----------

import plotly.express as px
px.histogram(df_genai_metrics, x="token_count", labels={"token_count": "Token Count"}, title="Distribution of Token Counts in Model Responses")

# COMMAND ----------

# Counting the occurrences of each answer correctness score
px.bar(df_genai_metrics['answer_correctness/v1/score'].value_counts(), title='Answer Correctness Score Distribution')

# COMMAND ----------

df_genai_metrics['toxicity'] = df_genai_metrics['toxicity/v1/score'] * 100
fig = px.scatter(df_genai_metrics, x='toxicity', y='answer_correctness/v1/score', title='Toxicity vs Correctness', size=[10]*len(df_genai_metrics))
fig.update_xaxes(tickformat=".2f")

# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC #Evaluation using RAGAS

# COMMAND ----------

# MAGIC %pip install -U -q langchain openai ragas arxiv pymupdf chromadb wandb tiktoken datasets mlflow==2.10.1 datasets tqdm databricks-vectorsearch==0.22 transformers torch langchain-together

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

from ragas.metrics import (
    answer_relevancy,
    faithfulness,
    context_recall,
    context_precision,
    context_relevancy,
    answer_correctness,
    answer_similarity
)

from ragas.metrics.critique import harmfulness
from ragas import evaluate
from datasets import Dataset
from tqdm import tqdm

# COMMAND ----------

# MAGIC %run "/Workspace/Repos/subramanian.narayana.ucalgary@gmail.com/databricks_hackathon_2024/config"

# COMMAND ----------

from mlflow import MlflowClient


# Helper function
def get_latest_model_version(model_name):
    mlflow_client = MlflowClient(registry_uri="databricks-uc")
    latest_version = 1
    for mv in mlflow_client.search_model_versions(f"name='{model_name}'"):
        version_int = int(mv.version)
        if version_int > latest_version:
            latest_version = version_int
    return latest_version

# COMMAND ----------

import mlflow
import os
os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get("dbdemos", "rag_sp_token")
model_name = f"{catalog}.{db}.asset_nav_chatbot_model_version_1"
model_version_to_evaluate = get_latest_model_version(model_name)
mlflow.set_registry_uri("databricks-uc")
rag_model = mlflow.langchain.load_model(f"models:/{model_name}/{model_version_to_evaluate}")

# COMMAND ----------



# COMMAND ----------

# from langchain_together import Together

# llm = Together(
#     model="togethercomputer/RedPajama-INCITE-7B-Base",
#     temperature=0.7,
#     max_tokens=128,
#     top_k=1,
#     together_api_key="ffc9a1325fe801c3244b69aedc57cbb8fd9969330b256a42ab9bf51678e5fd0e"
# )

# input_ = """You are a teacher with a deep knowledge of machine learning and AI. \
# You provide succinct and accurate answers. Answer the following question: 

# What is a large language model?"""
# print(llm.invoke(input_))

# COMMAND ----------

from langchain_together import Together
from langchain_together.embeddings import TogetherEmbeddings

together_key = "ffc9a1325fe801c3244b69aedc57cbb8fd9969330b256a42ab9bf51678e5fd0e"

together_embeddings = TogetherEmbeddings(model="togethercomputer/m2-bert-80M-8k-retrieval")

# together_completion = Together(
#     model="NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT",
#     temperature=0.7,
#     max_tokens=4000,
#     top_k=1,
#     together_api_key=together_key
# )

together_completion = Together(
    model="togethercomputer/RedPajama-INCITE-7B-Base",
    temperature=0.7,
    max_tokens=128,
    top_k=1,
    together_api_key="ffc9a1325fe801c3244b69aedc57cbb8fd9969330b256a42ab9bf51678e5fd0e"
)

input_ = """You are a teacher with a deep knowledge of machine learning and AI. \
You provide succinct and accurate answers. Answer the following question: 

What is a large language model?"""
print(together_completion.invoke(input_))

# COMMAND ----------

# dialog = {
#     "messages": [
#         {"role": "user", "content": "What is an Inverter?"}, 
#         {"role": "assistant", "content": "A power inverter, or invertor is a power electronic device or circuitry that changes direct current to alternating current. The resulting AC frequency obtained depends on the particular device employed."}, 
#         {"role": "user", "content": "What are all the fault codes in KACO inverter for overheating because of fans? and give me the ways to solve it."}
#     ]
# }
# print(f'Testing with relevant history and question...')
# response = rag_model.invoke(dialog)
# display_chat(dialog["messages"], response)

# COMMAND ----------

# response["result"]

# COMMAND ----------

def create_ragas_dataset(rag_model, eval_dataset):
  rag_dataset = []
  for index, row in tqdm(eval_dataset.iterrows()):
    answer = rag_model.invoke({"messages": [{"role": "user", "content": row["question"]}]})
    rag_dataset.append(
        {"question" : row["question"],
         "answer" : answer["result"],
         "contexts" : [row["context"]],
         "ground_truth" : row["ground_truth"]
         }
    )
  rag_df = pd.DataFrame(rag_dataset)
  rag_eval_dataset = Dataset.from_pandas(rag_df)
  return rag_eval_dataset

def evaluate_ragas_dataset(ragas_dataset, llm_model=together_completion, embeddings_model=together_embeddings):
  result = evaluate(
    ragas_dataset,
    metrics=[
        context_precision,
        faithfulness,
        answer_relevancy,
        context_recall,
        context_relevancy,
        answer_correctness,
        answer_similarity
    ],
    llm=llm_model, # together_completion
    embeddings=embeddings_model # embeddings
  )
  return result



# from langchain_together import Together
# from langchain_together.embeddings import TogetherEmbeddings

# together_key = "<your-key-here>"

# embeddings = TogetherEmbeddings(model="togethercomputer/m2-bert-80M-8k-retrieval")

# together_completion = Together(
#     model="NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT",
#     temperature=0.7,
#     max_tokens=4000,
#     top_k=1,
#     together_api_key=together_key
# )

# COMMAND ----------

from tqdm import tqdm
import pandas as pd
from datasets import Dataset

eval_dataset = pd.read_csv("/Volumes/main/asset_nav/volume_oem_documentation/evaluation_data/solar_rag_pipeline_evaluation_datatset_manual_with_context.csv").drop(columns="id")
display(eval_dataset)

basic_qa_ragas_dataset = create_ragas_dataset(rag_model, eval_dataset)

# COMMAND ----------

display(basic_qa_ragas_dataset)

# COMMAND ----------

basic_qa_result = evaluate_ragas_dataset(basic_qa_ragas_dataset)

# COMMAND ----------



# COMMAND ----------

# import mlflow.langchain
# model_version_uri = "models:/databricks_dbrx_models.models.dbrx_instruct/1"
# model_version_uri = "models:/databricks_dbrx_models.models.dbrx_base/1"
# model_version_uri = "models:/databricks_meta_llama_3_models.models.meta_llama_3_8b/1"
# model_version_uri = "models:/databricks_mistral_models.models.mistral_7b_v0_1/1"

# llm_model = mlflow.langchain.load_model(model_version_uri)

# COMMAND ----------

# import mlflow
# import pandas as pd

# mlflow.set_registry_uri("databricks-uc")

# logged_model = f"models:/databricks_meta_llama_3_models.models.meta_llama_3_8b/1"
# # Load model as a PyFuncModel.
# loaded_model = mlflow.pyfunc.load_model(logged_model)

# loaded_model._model_impl.pipeline("What is ML?")

# COMMAND ----------

# # code from ragas chatbot in the website

# from langchain.chat_models import AzureChatOpenAI
# from ragas.llms import LangchainLLM
# from ragas.metrics import faithfulness
# from ragas import evaluate

# # Initialize the AzureChatOpenAI model
# azure_model = AzureChatOpenAI(
#     deployment_name="your-deployment-name",
#     model="your-model-name",
#     openai_api_base="https://your-endpoint.openai.azure.com/",
#     openai_api_type="azure",
# )

# # Wrap the azure_model with LangchainLLM
# ragas_azure_model = LangchainLLM(azure_model)

# # Use the new model in the evaluate function
# result = evaluate(
#     your_dataset["eval"].select(range(10)),  # replace with your dataset
#     metrics=[faithfulness],
#     llm=ragas_azure_model,
# )

# print(result)

# COMMAND ----------

# MAGIC %md
# MAGIC #Model is PRD ready

# COMMAND ----------

client = MlflowClient()
client.set_registered_model_alias(name=model_name, alias="prod", version=model_version_to_evaluate)
